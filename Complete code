# # Descripción

# La compañía de seguros Sure Tomorrow quiere resolver varias tareas con la ayuda de machine learning y te pide que evalúes esa posibilidad.
# - Tarea 1: encontrar clientes que sean similares a un cliente determinado. Esto ayudará a los agentes de la compañía con el marketing.
# - Tarea 2: predecir la probabilidad de que un nuevo cliente reciba una prestación del seguro. ¿Puede un modelo de predictivo funcionar mejor que un modelo dummy?
# - Tarea 3: predecir el número de prestaciones de seguro que un nuevo cliente pueda recibir utilizando un modelo de regresión lineal.
# - Tarea 4: proteger los datos personales de los clientes sin afectar al modelo del ejercicio anterior. Es necesario desarrollar un algoritmo de transformación de datos que dificulte la recuperación de la información personal si los datos caen en manos equivocadas. Esto se denomina enmascaramiento u ofuscación de datos. Pero los datos deben protegerse de tal manera que no se vea afectada la calidad de los modelos de machine learning. No es necesario elegir el mejor modelo, basta con demostrar que el algoritmo funciona correctamente.
# 

# # Preprocesamiento y exploración de datos


pip install scikit-learn --upgrade


import numpy as np
import pandas as pd
import seaborn as sns
import math
import sklearn.linear_model
import sklearn.metrics
import sklearn.preprocessing
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from IPython.display import display


# Cargamos los datos y hacemos una revisión básica para comprobar que no hay problemas obvios.


df = pd.read_csv('/datasets/insurance_us.csv')

# Renombramos las columnas para que el código se vea más coherente con su estilo.

df = df.rename(columns={'Gender': 'gender', 'Age': 'age', 'Salary': 'income', 'Family members': 'family_members', 'Insurance benefits': 'insurance_benefits'})

df.sample(10)

df.info()


# Cambiaremos el tipo de datos de: age, por int


df['age']=df['age'].astype('int')


df.info()

# Echaremos un vistazo a las estadísticas descriptivas de los datos.

df.describe()


# ## Análisis exploratorio de datos

# Vamos a comprobar rápidamente si existen determinados grupos de clientes observando el gráfico de pares.

g = sns.pairplot(df, kind='hist')
g.fig.set_size_inches(12, 12)


# De acuerdo, es un poco complicado detectar grupos obvios (clústeres) ya que es difícil combinar diversas variables simultáneamente (para analizar distribuciones multivariadas). Ahí es donde LA y ML pueden ser bastante útiles.

# # Tarea 1. Clientes similares

# En el lenguaje de ML, es necesario desarrollar un procedimiento que devuelva los k vecinos más cercanos (objetos) para un objeto dado basándose en la distancia entre los objetos.
# 
# Para resolver la tarea, podemos probar diferentes métricas de distancia.

# Escribiremos una función que devuelva los k vecinos más cercanos para un $n^{th}$ objeto basándose en una métrica de distancia especificada. A la hora de realizar esta tarea no tendremos en cuenta el número de prestaciones de seguro recibidas.
# Lo probaremos para cuatro combinaciones de dos casos- Escalado
#   - los datos no están escalados
#   - los datos se escalan con el escalador MaxAbsScaler
# - Métricas de distancia
#   - Euclidiana
#   - Manhattan
# 


feature_names = ['gender', 'age', 'income', 'family_members']

def get_knn(df, n, k, metric):


    #param df: DataFrame de pandas utilizado para encontrar objetos similares dentro del mismo lugar
    #param n: número de objetos para los que se buscan los vecinos más cercanos
    #param k: número de vecinos más cercanos a devolver
    #param métrica: nombre de la métrica de distancia

    nbrs = NearestNeighbors(n_neighbors=k, metric=metric)
    nbrs.fit(df[feature_names])
    nbrs_distances, nbrs_indices = nbrs.kneighbors([df.iloc[n][feature_names]], k, return_distance=True)
    
    df_res = pd.concat([
        df.iloc[nbrs_indices[0]], 
        pd.DataFrame(nbrs_distances.T, index=nbrs_indices[0], columns=['distance'])
        ], axis=1)
    
    return df_res


# Escalamos los datos.

feature_names = ['gender', 'age', 'income', 'family_members']

transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(df[feature_names].to_numpy())

df_scaled = df.copy()
df_scaled.loc[:, feature_names] = transformer_mas.transform(df[feature_names].to_numpy())
df_scaled.sample(5)


# Ahora, vamos a obtener registros similares para uno determinado, para cada combinación


get_knn(df_scaled, 1, 10, 'euclidean')

get_knn(df_scaled, 1, 10, 'manhattan')

get_knn(df, 1, 10, 'euclidean')

get_knn(df, 1, 10, 'manhattan')


# Respuestas a las preguntas

# **¿El hecho de que los datos no estén escalados afecta al algoritmo kNN? Si es así, ¿cómo se manifiesta?** 
# 
# Si, al estar escalados, es más preciso, y lo vemos al comparar los datos de los clientes, mientras que los datos escalados son similares en todo, los no escalados, solo son iguales en el salario. Pero sobre todo lo notamos en insurance_benefits, pues es el que nos interesa, que los demás datos sean similares, para poder obtener un cliente similar 

# **¿Qué tan similares son los resultados al utilizar la métrica de distancia Manhattan (independientemente del escalado)?** 
# 
# Con los no datos escalados, en lo único que son iguales los datos es en el salario, pero el resto de los datos en realidad es bastante diferente, y de hecho "insurance_benefits" que es lo que nos interesa, todos son completamente diferentes
# Con los datos escalados, se parecen un poco más, aunque si comparamos las distancias con euclidean, vemos que son más cercanos los datos de euclidean 



# # Tarea 2. ¿Es probable que el cliente reciba una prestación del seguro?

# En términos de machine learning podemos considerarlo como una tarea de clasificación binaria.

# Con el valor de `insurance_benefits` superior a cero como objetivo, evaluaremos si el enfoque de clasificación kNN puede funcionar mejor que el modelo dummy.

# Construiremos un modelo dummy que es  simplemente un modelo aleatorio. Debería devolver "1" con cierta probabilidad. Probemos el modelo con cuatro valores de probabilidad: 0, la probabilidad de pagar cualquier prestación del seguro, 0.5, 1.
# La probabilidad de pagar cualquier prestación del seguro puede definirse como
# $$
# P\{\text{prestación de seguro recibida}\}=\frac{\text{número de clientes que han recibido alguna prestación de seguro}}{\text{número total de clientes}}.
# $$
# 
# Divide todos los datos correspondientes a las etapas de entrenamiento/prueba respetando la proporción 70:30.

# сalculamos el objetivo

df['insurance_benefits_received'] = df['insurance_benefits']>0

df

# comprueba el desequilibrio de clases con value_counts()
df.value_counts('insurance_benefits_received')


features= df.drop(['insurance_benefits_received', 'insurance_benefits'], axis=1).to_numpy()
target= df['insurance_benefits_received'].to_numpy()

features_train, features_valid, target_train, target_valid= train_test_split(features, target, test_size=.3, random_state=12345)

def eval_classifier(y_true, y_pred):
    
    f1_score = sklearn.metrics.f1_score(y_true, y_pred)
    print(f'F1: {f1_score:.2f}')
    
    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='all')
    print('Matriz de confusión')
    print(cm)



# generar la salida de un modelo aleatorio

def rnd_model_predict(P, size, seed=42):

    rng = np.random.default_rng(seed=seed)
    return rng.binomial(n=1, p=P, size=size)


for P in [0, df['insurance_benefits_received'].sum() / len(df), 0.5, 1]:

    print(f'La probabilidad: {P:.2f}')
    y_pred_rnd = rnd_model_predict(P, len(target), seed=42)
        
    eval_classifier(df['insurance_benefits_received'], y_pred_rnd)
    
    print()


# Creando el clasificador basado en KNN:

for i in range(1,11):
    knn_classifier = KNeighborsClassifier(n_neighbors=i)
    
    knn_classifier.fit(features_train, target_train)
    y_pred_knn = knn_classifier.predict(features_valid)
    print('n_neighbors:', i)
    print(eval_classifier(target_valid, y_pred_knn))


# Lo mismo pero con los datos escalados:

features_scaled= df_scaled.drop('insurance_benefits', axis=1)


features_train_sc, features_valid_sc, target_train, target_valid= train_test_split(features_scaled, target, test_size=.3, random_state=12345)


for i in range(1,11):
    knn_classifier = KNeighborsClassifier(n_neighbors=i)
    knn_classifier.fit(features_train_sc, target_train)
    y_pred_knn = knn_classifier.predict(features_valid_sc)
    print('n_neighbors:', i)
    print(eval_classifier(target_valid, y_pred_knn))


# Vemos que los datos escalados hacen que el modelo sea mucho más exacto, y con n_neighbors=1 alcanza casi la perfección, pues marca un .97 de f1_score



# # Tarea 3. Regresión (con regresión lineal)

# Con `insurance_benefits` como objetivo, evaluaremos cuál sería la RECM de un modelo de regresión lineal.

# Denotemos- $X$: matriz de características; cada fila es un caso, cada columna es una característica, la primera columna está formada por unidades- $y$ — objetivo (un vector)- $\hat{y}$ — objetivo estimado (un vector)- $w$ — vector de pesos
# La tarea de regresión lineal en el lenguaje de las matrices puede formularse así:
# $$
# y = Xw
# $$
# 
# El objetivo de entrenamiento es entonces encontrar esa $w$ que minimice la distancia L2 (ECM) entre $Xw$ y $y$:
# 
# $$
# \min_w d_2(Xw, y) \quad \text{or} \quad \min_w \text{MSE}(Xw, y)
# $$
# 
# Parece que hay una solución analítica para lo anteriormente expuesto:
# $$
# w = (X^T X)^{-1} X^T y
# $$
# 
# La fórmula anterior puede servir para encontrar los pesos $w$ y estos últimos pueden utilizarse para calcular los valores predichos
# $$
# \hat{y} = X_{val}w
# $$



class MyLinearRegression:
    
    def __init__(self):
        
        self.weights = None
    
    def fit(self, X, y):
        
        # añadir las unidades
        X2 = np.append(np.ones([len(X), 1]), X, axis=1)
        self.weights = np.linalg.inv(X2.T.dot(X2))@ X2.T @ y

    def predict(self, X):
        
        # añadir las unidades
        X2 = np.append(np.ones([len(X), 1]), X, axis=1)
        y_pred = X2.dot(self.weights)
        
        return y_pred


def eval_regressor(y_true, y_pred):
    
    rmse = math.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))
    print(f'RMSE: {rmse:.2f}')
    
    r2_score = sklearn.metrics.r2_score(y_true, y_pred)
    print(f'R2: {r2_score:.2f}')    


X = df[['age', 'gender', 'income', 'family_members']].to_numpy()
y = df['insurance_benefits'].to_numpy()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)

lr = MyLinearRegression()

lr.fit(X_train, y_train)
print(lr.weights)

y_test_pred = lr.predict(X_test)
eval_regressor(y_test, y_test_pred)


# Para los datos escalados:


X_sc = df_scaled[['age', 'gender', 'income', 'family_members']].to_numpy()
y_sc = df_scaled['insurance_benefits'].to_numpy()

X_train_sc, X_test_sc, y_train_sc, y_test_sc = train_test_split(X_sc, y_sc, test_size=0.3, random_state=12345)

lr_sc = MyLinearRegression()

lr_sc.fit(X_train_sc, y_train_sc)
print(lr_sc.weights)

y_test_pred_sc = lr_sc.predict(X_test_sc)
eval_regressor(y_test_sc, y_test_pred_sc)


# Vemos que el r2_score del modelo es igual tanto para los datos normales como para los datos escalados


# # Tarea 4. Ofuscar datos

# Lo mejor es ofuscar los datos multiplicando las características numéricas (recuerda que se pueden ver como la matriz $X$) por una matriz invertible $P$. 
# 
# $$
# X' = X \times P
# $$
# 


personal_info_column_list = ['gender', 'age', 'income', 'family_members']
df_pn = df[personal_info_column_list]


X = df_pn.to_numpy()


# Generar una matriz aleatoria $P$.

rng = np.random.default_rng(seed=42)
P = rng.random(size=(X.shape[1], X.shape[1]))


# Comprobar que la matriz P sea invertible

np.linalg.inv(P)


# Asi que ofuscaremos los datos multiplicando esta matriz por nuestros datos

ofs= X @ P


# ¿Puedes adivinar la edad o los ingresos de los clientes después de la transformación?

# No


# ¿Puedes recuperar los datos originales de $X'$ si conoces $P$? Intenta comprobarlo a través de los cálculos moviendo $P$ del lado derecho de la fórmula anterior al izquierdo. En este caso las reglas de la multiplicación matricial son realmente útiles

P_inv= np.linalg.inv(P)
X_recovered= ofs @ P_inv
X_recovered


# Muestra los tres casos para algunos clientes
# - Datos originales
# - El que está transformado
# - El que está invertido (recuperado)

# Datos_originales:

X[:3]


# Datos ofuscados (transformados):

ofs[:3]


# Datos recuperados:

X_recovered[:3]


# Seguramente puedes ver que algunos valores no son exactamente iguales a los de los datos originales. ¿Cuál podría ser la razón de ello?

# Podría ser por la precisión numérica y el redondeo al hacer los cálculos, pues transformamos los datos en repetidas ocsiones, lo que puede hcer que presenten ligeros cambios 

# ## Prueba de que la ofuscación de datos puede funcionar con regresión lineal

# En este proyecto la tarea de regresión se ha resuelto con la regresión lineal. Para demostrar que el método de ofuscación no afectará a la regresión lineal en términos de valores predichos, es decir, que sus valores seguirán siendo los mismos:
# Entonces, los datos están ofuscados y ahora tenemos $X \times P$ en lugar de tener solo $X$. En consecuencia, hay otros pesos $w_P$ como
# $$
# w = (X^T X)^{-1} X^T y \quad \Rightarrow \quad w_P = [(XP)^T XP]^{-1} (XP)^T y
# $$
# 

# **Respuesta**

# Utilizando la Reversibilidad de la transposición de un producto de matrices: $(AB)^T = B^TA^T$

# $w_P = [(XP)^T XP]^{-1} (XP)^T y \quad \Rightarrow \quad w_p=[P^T X^T X P]^{-1} P^T X^T y$
# 
# 
# Agrupamos las matrices del final:
# $w_p=[P^T X^T X P]^{-1} P^T X^T y \quad \Rightarrow \quad w_p=[P^T (X^T X P)]^{-1} P^T X^T y$
# 
# 
# Aplicamos las propiedades de la inversa:
# $w_p=[P^T (X^T X P)]^{-1} P^T X^T y \quad \Rightarrow \quad w_p= (X^T X P)^{-1} (P^T)^{-1} P^T X^T y$
# 
# Y con esto obtenemos una matriz identidad:
# $w_p= (X^T X P)^{-1} (P^T)^{-1} P^T X^T y \quad \Rightarrow \quad w_p= (X^T X P)^{-1} X^T y$
# 
# Si volvemos a aplicar las propiedades de la inversa, llegamos a que:
# $w_p=P^{-1}(X^TX)^{-1}X^Ty$

# Los valores predichos utilizando $w_p$ , serán los mismos que los valores predichos utilizando $w$ , ya que solo se están transformando junto con los datos ofuscados, con la matriz inversa
# Y esto significa que la cálidad será la misma para ambos pesos, el RECM no cambia 

# <table>
# <tr>
# <td>Distributividad</td><td>$A(B+C)=AB+AC$</td>
# </tr>
# <tr>
# <td>No conmutatividad</td><td>$AB \neq BA$</td>
# </tr>
# <tr>
# <td>Propiedad asociativa de la multiplicación</td><td>$(AB)C = A(BC)$</td>
# </tr>
# <tr>
# <td>Propiedad de identidad multiplicativa</td><td>$IA = AI = A$</td>
# </tr>
# <tr>
# <td></td><td>$A^{-1}A = AA^{-1} = I$
# </td>
# </tr>    
# <tr>
# <td></td><td>$(AB)^{-1} = B^{-1}A^{-1}$</td>
# </tr>    
# <tr>
# <td>Reversibilidad de la transposición de un producto de matrices,</td><td>$(AB)^T = B^TA^T$</td>
# </tr>    
# </table>

# ## Prueba de regresión lineal con ofuscación de datos

# Ahora, probemos que la regresión lineal pueda funcionar, en términos computacionales, con la transformación de ofuscación elegida.
# Construiremos un procedimiento que ejecute la regresión lineal opcionalmente con la ofuscación.
# Ejecutaremos la regresión lineal para los datos originales y los ofuscados, comparararemos los valores predichos y los valores de las métricas RMSE y $R^2$.

# **Procedimiento**
# 
# - Creamos una matriz cuadrada $P$ de números aleatorios.- Comprobamos que sea invertible.
# - Utilizamos $XP$ como la nueva matriz de características

# Probamos el modelo con los datos originales:

model_lr= LinearRegression()


model_lr.fit(X_train, y_train)
predictions= model_lr.predict(X_test)

rmse_or = math.sqrt(sklearn.metrics.mean_squared_error(y_test, predictions))
print('RMSE original: ', rmse_or)

r2_score_or = sklearn.metrics.r2_score(y_test, predictions)
print('R2 original: ', r2_score_or)   


# Ahora con los datos ofuscados:


X = df[['age', 'gender', 'income', 'family_members']].to_numpy()
y = df['insurance_benefits'].to_numpy()

P_check= np.random.random(size=(X.shape[1], X.shape[1]))
ofs_check= X @ P_check

ofs_train, ofs_test, y_train, y_test = train_test_split(ofs_check, y, test_size=0.3, random_state=12345)



model_lr_ofs= LinearRegression()

model_lr_ofs.fit(ofs_train, y_train)
predictions_ofs= model_lr_ofs.predict(ofs_test)

rmse_or = math.sqrt(sklearn.metrics.mean_squared_error(y_test, predictions_ofs))
print('RMSE ofuscado: ', rmse_or)

r2_score_or = sklearn.metrics.r2_score(y_test, predictions_ofs)
print('R2 ofuscado: ', r2_score_or)   


# Vemos que el RMSE y el R2 son iguales tanto para los datos originales com para los ofuscados, pues como vimos anteriormente, al ofuscar los datos, los multiplcamos por cierta matriz invertible, y se hace lo mismo con los pesos, de esa manera se obtiene el mismo resultado para ambos datos 

# # Conclusiones

# Creamos una función para encontrar clientes que sean similares a un cliente determinado. Esto ayudará a los agentes de la compañía con el marketing.
# 
# Creamos un calsificador basado en KNN para predecir la probabilidad de que un nuevo cliente reciba una prestación del seguro. Y lo calificamos ycomparamos con un modelo dummy
# Creamos un modelo de regresión lineal desde cero para predecir el número de prestaciones de seguro que un nuevo cliente pueda recibir
# Ofuscamos los datos para proteger los datos personales de los clientes sin afectar al modelo. 
# Y demostramos que el modelo predice y trabaja igual con datos ofuscados y con los datos originales
